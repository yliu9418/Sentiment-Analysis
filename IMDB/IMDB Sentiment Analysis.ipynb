{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPU.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOJXqwswD/rlJya2rvcUAAH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"I1XbrHK0nrLX","colab":{"base_uri":"https://localhost:8080/","height":666},"executionInfo":{"status":"ok","timestamp":1593309507827,"user_tz":240,"elapsed":3725,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"e98bfa2a-f560-4fd5-c5a2-a0a489b40a67"},"source":[" !pip install tensorflow-gpu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n","Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.30.0)\n","Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (47.3.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Al7OohEwiCmD","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593309514198,"user_tz":240,"elapsed":2958,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"7eb11a75-159f-4ff7-8e5d-d2c4763523ad"},"source":["from __future__ import division, print_function\n","from gensim import models\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n","from keras.layers.recurrent import LSTM\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","import os\n","import collections\n","import re\n","import string"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CWhAta9FiC31"},"source":["data = pd.read_csv('abc/IMDB Dataset 2.tsv', header = None, delimiter='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GADCOlHoiC-6"},"source":["data.columns = ['Text', 'Label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEQl2NzwiDCb","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593309519143,"user_tz":240,"elapsed":2533,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"715159e5-8052-4686-e1cb-ed3e0e7e2565"},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Text  Label\n","0  One of the other reviewers has mentioned that ...      1\n","1  A wonderful little production. <br /><br />The...      1\n","2  I thought this was a wonderful way to spend ti...      1\n","3  Basically there's a family where a little boy ...      0\n","4  Petter Mattei's \"Love in the Time of Money\" is...      1"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"gIsIDzeeiDHH"},"source":["pos = []\n","neg = []\n","for l in data.Label:\n","    if l == 0:\n","        pos.append(0)\n","        neg.append(1)\n","    elif l == 1:\n","        pos.append(1)\n","        neg.append(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMFWUdiMiDKh"},"source":["data['Pos']= pos\n","data['Neg']= neg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2G9tD7qiDRo","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1593309521786,"user_tz":240,"elapsed":1754,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"65a61f60-809f-4d47-82f8-51f2fd7349ae"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"Ndi45X18iDOR","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1593309582793,"user_tz":240,"elapsed":62106,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"888f4336-7e6d-4117-97d9-268541962307"},"source":["# Clean Data\n","def remove_punct(text):\n","    text_nopunct = ''\n","    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n","    return text_nopunct\n","\n","data['Text_Clean'] = data['Text'].apply(lambda x: remove_punct(x))\n","from nltk import word_tokenize, WordNetLemmatizer\n","tokens = [word_tokenize(sen) for sen in data.Text_Clean]\n","def lower_token(tokens): \n","    return [w.lower() for w in tokens]    \n","    \n","lower_tokens = [lower_token(token) for token in tokens]\n","from nltk.corpus import stopwords\n","stoplist = stopwords.words('english')\n","\n","def remove_stop_words(tokens):\n","    return [word for word in tokens if word not in stoplist]\n","filtered_words = [remove_stop_words(sen) for sen in lower_tokens]\n","result = [' '.join(sen) for sen in filtered_words]\n","data['Text_Final'] = result\n","data['tokens'] = filtered_words\n","data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]\n","data[:4]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text_Final</th>\n","      <th>tokens</th>\n","      <th>Label</th>\n","      <th>Pos</th>\n","      <th>Neg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one reviewers mentioned watching 1 oz episode ...</td>\n","      <td>[one, reviewers, mentioned, watching, 1, oz, e...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wonderful little production br br filming tech...</td>\n","      <td>[wonderful, little, production, br, br, filmin...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>thought wonderful way spend time hot summer we...</td>\n","      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>basically theres family little boy jake thinks...</td>\n","      <td>[basically, theres, family, little, boy, jake,...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          Text_Final  ... Neg\n","0  one reviewers mentioned watching 1 oz episode ...  ...   0\n","1  wonderful little production br br filming tech...  ...   0\n","2  thought wonderful way spend time hot summer we...  ...   0\n","3  basically theres family little boy jake thinks...  ...   1\n","\n","[4 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Bxrc8lQcku15","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593309584136,"user_tz":240,"elapsed":62794,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"583e0374-fc13-4f0b-ce4d-9c3c65519879"},"source":["# Split Data into test and train\n","data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)\n","all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n","training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n","TRAINING_VOCAB = sorted(list(set(all_training_words)))\n","print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n","print(\"Max sentence length is %s\" % max(training_sentence_lengths))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5521847 words total, with a vocabulary size of 170579\n","Max sentence length is 1449\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qWwNB4mJku5r","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593309585528,"user_tz":240,"elapsed":1373,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"305fc8ec-784d-4847-e2f5-b40efa3ae13d"},"source":["all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n","test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n","TEST_VOCAB = sorted(list(set(all_test_words)))\n","print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n","print(\"Max sentence length is %s\" % max(test_sentence_lengths))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["619451 words total, with a vocabulary size of 51872\n","Max sentence length is 594\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uS9mqyekku9M","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1593309672366,"user_tz":240,"elapsed":9576,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"1f2e2dd7-13d0-4e11-d1e6-03d0bfc8b918"},"source":["#word2vec_path = '/Users/susman/Desktop/glove_Reddit_200d.txt' #train Glove\n","word2vec_path = 'glove_wiki_300d.txt' #train Glove\n","word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, encoding=\"ISO-8859-1\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xiqBU0Gkku_-"},"source":["def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n","    if len(tokens_list)<1:\n","        return np.zeros(k)\n","    if generate_missing:\n","        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n","    else:\n","        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n","    length = len(vectorized)\n","    summed = np.sum(vectorized, axis=0)\n","    averaged = np.divide(summed, length)\n","    return averaged\n","\n","def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n","    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n","                                                                                generate_missing=generate_missing))\n","    return list(embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItdmL6q4nMdu"},"source":["training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTr3kUZKnMre"},"source":["MAX_SEQUENCE_LENGTH = 500 # change length \n","EMBEDDING_DIM = 300"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWH5p0kInR8-","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593309693882,"user_tz":240,"elapsed":21472,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"32b84fe7-36cb-488b-f938-e6e48e0a6b50"},"source":["# Tokenize and Pad sequences\n","tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n","tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n","training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n","\n","train_word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(train_word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 170564 unique tokens.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GTN9noiqnSCI"},"source":["train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUlur8KLnSFs","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593309696914,"user_tz":240,"elapsed":2976,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"5c0a898a-e86f-458c-805d-e41dc3425c58"},"source":["train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n","for word,index in train_word_index.items():\n","    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n","print(train_embedding_weights.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(170565, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tfFHPreLnSIe"},"source":["test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n","test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ntOK_sinSL0","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1593309696917,"user_tz":240,"elapsed":2948,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"53056bce-6a7c-42e0-b96f-5dc42e731ab4"},"source":["# Define RNN-LSTM\n","from keras.layers import Bidirectional\n","from keras import regularizers\n","\n","label_names = ['Pos', 'Neg']\n","y_train = data_train[label_names].values\n","x_train = train_cnn_data\n","y_tr = y_train\n","def recurrent_nn(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n","    \n","    #embedding_layer = Embedding(num_words,\n","     #                       embedding_dim,\n","      #                      weights=[embeddings],\n","       #                     input_length=max_sequence_length,\n","        #                    trainable=False)\n","    embedding_layer = Embedding(num_words,\n","                            embedding_dim,\n","                            input_length=max_sequence_length,\n","                            trainable=True)\n","    \n","    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","\n","#     lstm = LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedded_sequences)\n","    #lstm = LSTM(256, kernel_initializer = 'glorot_uniform')(embedded_sequences)\n","    lstm = Bidirectional(LSTM(200, kernel_initializer = 'glorot_uniform',kernel_regularizer=regularizers.l2(0.)))(embedded_sequences)\n","    \n","   # lstm = Bidirectional(LSTM(256, kernel_initializer = 'glorot_uniform'))(embedded_sequences)\n","    \n","    x = Dense(128, activation='relu', kernel_initializer = 'glorot_uniform',kernel_regularizer=regularizers.l2(0.))(lstm)\n","    x = Dropout(0.1)(x)\n","    preds = Dense(labels_index, activation='sigmoid')(x)\n","\n","    model = Model(sequence_input, preds)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['acc'])\n","    model.summary()\n","    return model\n","\n","\n","model = recurrent_nn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n","                len(list(label_names)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 500)               0         \n","_________________________________________________________________\n","embedding_2 (Embedding)      (None, 500, 300)          51169500  \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 400)               801600    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 128)               51328     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 2)                 258       \n","=================================================================\n","Total params: 52,022,686\n","Trainable params: 52,022,686\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RMdqrSKcnSPi","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593309697539,"user_tz":240,"elapsed":3551,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"1d0a4b9f-69ad-4b27-c725-c1f3eab69c88"},"source":["# check the shape \n","print(x_train.shape, y_tr.shape)\n","print(test_cnn_data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(45000, 500) (45000, 2)\n","(5000, 500)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CP_o5DS0nSS1","colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"status":"ok","timestamp":1593316037597,"user_tz":240,"elapsed":6343590,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"c8355a6b-4843-43f5-d643-5afb79c20010"},"source":["# Train RNN-LSTM\n","import keras \n","import os\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","num_epochs = 80\n","batch_size = 128\n","earlystopper = keras.callbacks.EarlyStopping(patience=15, verbose=1)\n","history = keras.callbacks.callbacks.History()\n","\n","hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.3, shuffle=True, batch_size=batch_size, \n","                 callbacks=[earlystopper, history])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 31499 samples, validate on 13501 samples\n","Epoch 1/80\n","31499/31499 [==============================] - 412s 13ms/step - loss: 0.3717 - acc: 0.8305 - val_loss: 0.2794 - val_acc: 0.8878\n","Epoch 2/80\n","31499/31499 [==============================] - 403s 13ms/step - loss: 0.1240 - acc: 0.9578 - val_loss: 0.2986 - val_acc: 0.8872\n","Epoch 3/80\n","31499/31499 [==============================] - 394s 13ms/step - loss: 0.0598 - acc: 0.9802 - val_loss: 0.4174 - val_acc: 0.8796\n","Epoch 4/80\n","31499/31499 [==============================] - 394s 13ms/step - loss: 0.0327 - acc: 0.9896 - val_loss: 0.5681 - val_acc: 0.8697\n","Epoch 5/80\n","31499/31499 [==============================] - 393s 12ms/step - loss: 0.0279 - acc: 0.9909 - val_loss: 0.5871 - val_acc: 0.8719\n","Epoch 6/80\n","31499/31499 [==============================] - 403s 13ms/step - loss: 0.0243 - acc: 0.9921 - val_loss: 0.6363 - val_acc: 0.8685\n","Epoch 7/80\n","31499/31499 [==============================] - 402s 13ms/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.7174 - val_acc: 0.8524\n","Epoch 8/80\n","31499/31499 [==============================] - 396s 13ms/step - loss: 0.0336 - acc: 0.9888 - val_loss: 0.6692 - val_acc: 0.8608\n","Epoch 9/80\n","31499/31499 [==============================] - 394s 12ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.7854 - val_acc: 0.8670\n","Epoch 10/80\n","31499/31499 [==============================] - 391s 12ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.8560 - val_acc: 0.8529\n","Epoch 11/80\n","31499/31499 [==============================] - 392s 12ms/step - loss: 0.0157 - acc: 0.9952 - val_loss: 0.7650 - val_acc: 0.8525\n","Epoch 12/80\n","31499/31499 [==============================] - 394s 13ms/step - loss: 0.0111 - acc: 0.9966 - val_loss: 0.9758 - val_acc: 0.8615\n","Epoch 13/80\n","31499/31499 [==============================] - 393s 12ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.8886 - val_acc: 0.8596\n","Epoch 14/80\n","31499/31499 [==============================] - 392s 12ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 1.0607 - val_acc: 0.8609\n","Epoch 15/80\n","31499/31499 [==============================] - 394s 13ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.8669 - val_acc: 0.8416\n","Epoch 16/80\n","31499/31499 [==============================] - 391s 12ms/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.8960 - val_acc: 0.8533\n","Epoch 00016: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2bMQEUO6nrQR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1593318035868,"user_tz":240,"elapsed":3778,"user":{"displayName":"Yang Liu","photoUrl":"","userId":"13097024656060116044"}},"outputId":"59363de2-4a89-4157-bfef-7986bd7f4665"},"source":["# Test RNN-LSTM\n","predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5000/5000 [==============================] - 3s 536us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sZwAXzRLnrVJ"},"source":["from sklearn import metrics\n","metrics.confusion_matrix(data_test.Label, prediction_labels)\n","\n","y_test = data_test.Label\n","y_pred_class = prediction_labels\n","# save confusion matrix and slice into four pieces\n","confusion = metrics.confusion_matrix(y_test, y_pred_class)\n","print(confusion)\n","#[row, column]\n","TP = confusion[1, 1]\n","TN = confusion[0, 0]\n","FP = confusion[0, 1]\n","FN = confusion[1, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsK7-taa5UA_"},"source":["sensitivity = TP / float(FN + TP)\n","\n","print(sensitivity)\n","print(metrics.recall_score(y_test, y_pred_class))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQ3Agm5y5WkW"},"source":["specificity = TN / (TN + FP)\n","\n","print(specificity)"],"execution_count":null,"outputs":[]}]}